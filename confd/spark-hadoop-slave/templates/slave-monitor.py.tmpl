import sys
import urllib2
import json
import re

DFS_ROLE_MASTER = "hdfs-master"
DFS_ROLE_SLAVE = "hdfs-slave"
YARN_ROLE_MASTER = "yarn-master"
YARN_ROLE_SlAVE = "hadoop-slave"
SPARK_ROLE_MASTER = "spark-master"
SPARK_ROLE_WORKER = "spark-worker"

def parse_dfs_stat(role, hjson, stats):    
    if "beans" not in hjson:
        return
    if role == DFS_ROLE_MASTER:
        for sub_dict in hjson["beans"]:
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=NameNodeActivity":
                stats["FilesCreated"] = sub_dict["FilesCreated"]
                stats["FilesAppended"] = sub_dict["FilesAppended"]
		stats["FilesRenamed"] = sub_dict["FilesRenamed"]
                stats["FilesDeleted"] = sub_dict["FilesDeleted"]
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=FSNamesystem":
		stats["CapacityTotalGB"] = sub_dict["CapacityTotalGB"]
		stats["UsedGB"] = sub_dict["CapacityUsedGB"]
		stats["RemainingGB"] = sub_dict["CapacityRemainingGB"]
		stats["FilesTotal"] = sub_dict["FilesTotal"]
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=FSNamesystemState":
		stats["LiveNodes"] = sub_dict["NumLiveDataNodes"]
		stats["DeadNodes"] = sub_dict["NumDeadDataNodes"]
		stats["DecomLiveNodes"] = sub_dict["NumDecomLiveDataNodes"]
		stats["DecomDeadNodes"] = sub_dict["NumDecomDeadDataNodes"]
		stats["DecommissioningNodes"] = sub_dict["NumDecommissioningDataNodes"]
		stats["StaleNodes"] = sub_dict["NumStaleDataNodes"]
		stats["StaleStorages"] = sub_dict["NumStaleStorages"]
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=NameNodeInfo":
                stats["PercentUsed"] = sub_dict["PercentUsed"]
		stats["PercentRemaining"] = sub_dict["PercentRemaining"]                
    elif role == DFS_ROLE_SLAVE:
	for sub_dict in hjson["beans"]:
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=DataNode,name=FSDatasetState":
		stats["RemainingGB"] = sub_dict["Remaining"] / (1024*1024*1024)
		stats["DfsUsedGB"] = sub_dict["DfsUsed"] / (1024*1024*1024)
	    if "name" in sub_dict and re.match(r'Hadoop:service=DataNode,name=DataNodeActivity-.*', sub_dict["name"]):
		stats["BlocksWritten"] = sub_dict["BlocksWritten"]
		stats["BlocksRead"] = sub_dict["BlocksRead"]
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=DataNode,name=JvmMetrics":
		stats["GcCount"] = sub_dict["GcCount"]
		stats["GcNumWarnThresholdExceeded"] = sub_dict["GcNumWarnThresholdExceeded"]

def parse_yarn_stat(role, hjson, stats):    
    if "beans" not in hjson:
        return
    if role == YARN_ROLE_MASTER:
        for sub_dict in hjson["beans"]:
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=ResourceManager,name=ClusterMetrics":
                stats["NumActiveNMs"] = sub_dict["NumActiveNMs"]
                stats["NumDecommissionedNMs"] = sub_dict["NumDecommissionedNMs"]
                stats["NumLostNMs"] = sub_dict["NumLostNMs"]
                stats["NumUnhealthyNMs"] = sub_dict["NumUnhealthyNMs"]
                stats["NumRebootedNMs"] = sub_dict["NumRebootedNMs"]
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=ResourceManager,name=QueueMetrics,q0=root":
                stats["running_0"] = sub_dict["running_0"]
                stats["running_60"] = sub_dict["running_60"]
                stats["running_300"] = sub_dict["running_300"]
                stats["running_1440"] = sub_dict["running_1440"]
                stats["AppsSubmitted"] = sub_dict["AppsSubmitted"]
                stats["AppsRunning"] = sub_dict["AppsRunning"]
                stats["AppsPending"] = sub_dict["AppsPending"]
                stats["AppsCompleted"] = sub_dict["AppsCompleted"]
                stats["AppsFailed"] = sub_dict["AppsFailed"]
                stats["AllocatedMB"] = sub_dict["AllocatedMB"]
                stats["AvailableMB"] = sub_dict["AvailableMB"]
                stats["ReservedMB"] = sub_dict["ReservedMB"]
                stats["AllocatedVCores"] = sub_dict["AllocatedVCores"]
                stats["AvailableVCores"] = sub_dict["AvailableVCores"]
                stats["ReservedVCores"] = sub_dict["ReservedVCores"]
                stats["AllocatedContainers"] = sub_dict["AllocatedContainers"]
                stats["ReservedContainers"] = sub_dict["ReservedContainers"]
    elif role == YARN_ROLE_SlAVE:
        for sub_dict in hjson["beans"]:
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NodeManager,name=NodeManagerMetrics":
                stats["ContainersIniting"] = sub_dict["ContainersIniting"]
                stats["ContainersRunning"] = sub_dict["ContainersRunning"]
                stats["AllocatedContainers"] = sub_dict["AllocatedContainers"]
                stats["AllocatedGB"] = sub_dict["AllocatedGB"]
                stats["AvailableGB"] = sub_dict["AvailableGB"]

def parse_spark_stat(role, hjson, stats):    
    if "gauges" not in hjson:
        return	
    if role == SPARK_ROLE_MASTER:
	stats["WorkersTotal"] = hjson["gauges"]["master.workers"]["value"]
	stats["WorkersAlive"] = hjson["gauges"]["master.aliveWorkers"]["value"]
	stats["RunningApps"] = hjson["gauges"]["master.apps"]["value"]
	stats["WaitingApps"] = hjson["gauges"]["master.waitingApps"]["value"]
    elif role == SPARK_ROLE_WORKER:
	stats["CoresFree"] = hjson["gauges"]["worker.coresFree"]["value"]
	stats["CoresUsed"] = hjson["gauges"]["worker.coresUsed"]["value"]
	stats["Executors"] = hjson["gauges"]["worker.executors"]["value"]
	stats["MemFreeMB"] = hjson["gauges"]["worker.memFree_MB"]["value"]
	stats["MemUsedMB"] = hjson["gauges"]["worker.memUsed_MB"]["value"]

if __name__ == '__main__':
    ip = '{{getv "/host/ip"}}'
    stats = {}
	
    role = DFS_ROLE_SLAVE
    port = "9864"
    res = urllib2.urlopen("http://%s:%s/jmx?qry=Hadoop:service=DataNode,name=*" % (ip, port))
    pjson = json.loads(res.read())
    parse_dfs_stat(role, pjson, stats)

    role = YARN_ROLE_SlAVE
    port = "8042"
    res = urllib2.urlopen("http://%s:%s/jmx?qry=Hadoop:service=NodeManager,name=NodeManagerMetrics" % (ip, port))
    pjson = json.loads(res.read())
    parse_yarn_stat(role, pjson, stats)
{{if eq (getv "/env/enable_spark_standalone") "true"}}
    role = SPARK_ROLE_WORKER
    port = "8081"
    res = urllib2.urlopen("http://%s:%s/metrics/json/" % (ip, port))
    pjson = json.loads(res.read())
    parse_spark_stat(role, pjson, stats)
{{end}}	
    print json.dumps(stats)
